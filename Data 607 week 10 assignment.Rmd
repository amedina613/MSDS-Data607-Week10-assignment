---
title: "Week 10 assignment"
author: "Adriana Medina"
date: "2024-03-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Introduction: 
In Text Mining with R, Chapter 2 looks at Sentiment Analysis.  In this assignment, you should start by getting the primary example code from chapter 2 working in an R Markdown document.  You should provide a citation to this base code.  You’re then asked to extend the code in two ways:
Work with a different corpus of your choosing, and incorporate at least one additional sentiment lexicon (possibly from another R package that you’ve found through research).

The base code for this assignment is from chapter 2 of Text Mining with R: A Tidy Approach https://www.tidytextmining.com/sentiment.html


```{r}
library(janeaustenr)
library(dplyr)
library(stringr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(sentimentr)
```

# Sentiment analysis with inner join

```{r example code}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

glimpse(tidy_books)
```
```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")

```

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")
```

```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```
```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```
# Most common positive and negative words
```{r}
bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```
# New Data Set
```{r new book}
library(gutenbergr)
```
```{r}
gutenberg_metadata
```
__Using gutenberg library, I was curious if they had one of my favorite books, so I used gutenberg_metadata and grep function to look for the title__
```{r}
# Search for the book by title
book_title <- "Strange Case of Dr. Jekyll and Mr. Hyde"
drj_info <- gutenberg_metadata[grep(book_title, gutenberg_metadata$title), ] #use partial match with grep function

print(drj_info)
```
```{r}
DrJekyll <- gutenberg_download(42)
```
__Tidy Data__
```{r}
tidy_DrJekyll <- DrJekyll %>%
  mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", ignore_case = TRUE)))) %>%
  unnest_tokens(word, text)
```

# Analysis 

__Let's compare the positive and negative words in bing lexicon and the loughran lexicon__
```{r}
#filtering the loughran lexicon only for positive an negative words
loughran_posneg <- get_sentiments("loughran") %>% 
  filter(sentiment == "positive" | sentiment =="negative")

#creating net sentiment using bing
DrJekyll_bing <- tidy_DrJekyll %>%
  inner_join(get_sentiments("bing")) %>%
  count(index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
#creating net sentiment using loughran
DrJekyll_loughran <- tidy_DrJekyll %>%
  inner_join(loughran_posneg) %>%
  count(index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

```{r}
par(mfrow=c(1,2))

ggplot(DrJekyll_loughran, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Sentiment Analysis of Dr. Jekyll using Loughran Lexicon")
```

```{r}
ggplot(DrJekyll_bing, aes(index, sentiment)) +
  geom_col(show.legend = FALSE) +
  labs(title = "Sentiment Analysis of Dr. Jekyll using Bing Lexicon")
```

# Conclusion

__It seems as though there is a disparity in positive/negative words identified using the different lexicons.__

```{r}
get_sentiments("loughran") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)
```
```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```

__This could be due to the difference in the negative and positive sentiments in the lexicons themselves. It is necessary to make this observation when conducting analysis because it could affect our results.__ 
